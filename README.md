# Data-Analysis-using-Pyspark
<h2>A data analysis repository using pyspark.</h2>
<p>.As a data analyst you should be able to apply different queries to your dataset to extract useful information out of it. but what if your data is so big that working with it on your local machine is not easy to be done. That is when the distributed data processing and Spark Technology will become handy. So in this project we are going to work with pyspark module in python and we are going to use google colab environment in order to apply some queries to the dataset we have related to lastfm website which is an online music service where users can listen to different songs. This dataset is containing two csv files listening.csv and genre.csv. Also we will learn how we can visualize our query results using matplotlib.</p>
<h3>Task 1: Introduction</h3>
<p>Understand the purpose of the project, the datasets that will be used, and the things we are going to learn at the end of this project.</p>

<h3>Task 2: Importing first csv file of the dataset</h3>
<p>Import listening.csv file, which contains data related to different users listened to different songs and prepare it for the analysis by dropping columns and deleting null values.</p>.
<h3>Task 3: Applying some Queries</h3>
<p>Apply some queries to this dataset to extract useful information out of it. 
 We will start from simple queries to more advanced ones.</p>

<h3>Task 4: Importing second csv file of the dataset</h3>
<p>Importing genre.csv file and merge it with listening.csv to answer more advanced queries</p>

<h3>Task 5: Visualizing the query results</h3>
<p>Learn how to visualize our query results using matplotlib. </p>

<h2>In this course, we are going to focus on 3 learning objectives:</h2>

<ol>Learn how to setup the google colab for distributed data processing </ol>

<ol>Learn applying different queries to your dataset to extract useful Information</ol>

<ol>Learn how to visualize this information using matplotlib </ol>

<h2>Pyspark</h2>
<p>PySpark is the Python API for Apache Spark. It enables you to perform real-time, large-scale data processing in a distributed environment using Python. It also provides a PySpark shell for interactively analyzing your data.

PySpark combines Python’s learnability and ease of use with the power of Apache Spark to enable processing and analysis of data at any size for everyone familiar with Python.

PySpark supports all of Spark’s features such as Spark SQL, DataFrames, Structured Streaming, Machine Learning (MLlib) and Spark Core.</p>

<h3><b>Link to the official Documentation: https://spark.apache.org/docs/latest/api/python/index.html</b></h3>
